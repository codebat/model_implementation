{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tries to predict the prices of booking hotels after learning embeddings of dates and using historical booking prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have embeddings for Weekday, Month and Date.  Make a sequential model for price prediction in 2020 Feb.  Use transformers for prediction of future prices. \n",
    "Explanation for Weekday:\n",
    "- Saturday and Sunday are supposed to more pricy (higher demand) than others\n",
    "- Adjacent days are also supposed to be more pricy \n",
    "\n",
    "Explanation for Month:\n",
    "- Festival might be associated with months (like christmas)\n",
    "- Months might also relate to weather. Some places might have extreme weathers which will result in lower demand in those weather\n",
    "\n",
    "Explanation for Date:\n",
    "- Day of the month might also be associated with festivals\n",
    "- People might be more willing to travel during the start or end of the month depending on the work pattern. One such examples would be During the tax filings or financial year closing time which is towards end of the month less people are likely to travel from a city which is working hub\n",
    "\n",
    "Explanation for not including year\n",
    "- If there are some event which are happening in a particular year (like hosting a world cup) we better avoid incorporating such information in our model\n",
    "- Exception to this might be repititive events which repeats at a frequency of multiplicity of a year. But this has been ignored in preparation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, nhead, dim_feedforward, num_weekday, num_months, num_days, num_price_bucket, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_weekday = num_weekday\n",
    "        self.num_months = num_months\n",
    "        self.num_days = num_days\n",
    "        self.num_price_bucket = num_price_bucket\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, dropout, self.seq_len)\n",
    "        transformer_encoder_layer = TransformerEncoderLayer(self.d_model, self.nhead, self.dim_feedforward)\n",
    "        transformer_decoder_layer = TransformerDecoderLayer(self.d_model, self.nhead, self.dim_feedforward)\n",
    "        self.encoder = TransformerEncoder(transformer_encoder_layer, 2)\n",
    "        self.decoder = TransformerDecoder(transformer_decoder_layer, 2)\n",
    "        self.weekday_embedding = nn.Embedding(num_weekday, self.d_model)\n",
    "        self.month_embedding = nn.Embedding(num_months, self.d_model)\n",
    "        self.num_days_embedding = nn.Embedding(num_days, self.d_model)\n",
    "        self.price_bucket_embedding = nn.Embedding(self.num_price_bucket, self.d_model)\n",
    "        self.linear = nn.Linear(d_model,1)\n",
    "        self.mask = torch.triu(torch.ones(self.seq_len, self.seq_len), diagonal=1)\n",
    "        self.mask[self.mask==1]=float('-inf')\n",
    "        \n",
    "    def forward(self, enc_inp, dec_inp):\n",
    "        enc_inp = self.month_embedding(enc_inp[...,0])+self.num_days_embedding(enc_inp[...,1])+self.weekday_embedding(enc_inp[...,2])\n",
    "        dec_inp = self.price_bucket_embedding(dec_inp)\n",
    "        enc_inp = enc_inp.transpose(0,-2)\n",
    "        dec_inp = dec_inp.transpose(0,-2)\n",
    "        enc_inp = self.pos_encoder(enc_inp)\n",
    "        dec_inp = self.pos_encoder(dec_inp)\n",
    "        enc_out = self.encoder(enc_inp)\n",
    "        dec_out = self.decoder(tgt=dec_inp, memory=enc_out, tgt_mask=self.mask)\n",
    "        output = self.linear(dec_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 7\n",
    "class priceDataset(Dataset):\n",
    "    def __init__(self, price_df):\n",
    "        self.data = price_df\n",
    "        self.default_price_bucket = self.data['price_bucket'].mode()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)-SEQLEN\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        enc_inp = torch.tensor(self.data[['month', 'day', 'weekday']][index:index+SEQLEN].values)\n",
    "        response = torch.tensor(self.data['Price'][index:index+SEQLEN].values,dtype=torch.float32)\n",
    "        if index:\n",
    "            dec_inp = torch.tensor(self.data['price_bucket'][index:index+SEQLEN].values)\n",
    "        else:\n",
    "            dec_inp = torch.tensor(np.append(self.default_price_bucket,self.data['price_bucket'][index:index+SEQLEN-1]))\n",
    "        if dec_inp.shape[0]<SEQLEN:\n",
    "            dec_inp = F.pad(dec_inp, pad=(0,SEQLEN-dec_inp.shape[0]), mode='constant', value=0)\n",
    "            enc_inp = F.pad(enc_inp, pad=(0,0,0,SEQLEN-dec_inp.shape[0]), mode='constant', value=0)\n",
    "            response = F.pad(response, pad=(0,SEQLEN-dec_inp.shape[0]), mode='constant', value=0)\n",
    "        return enc_inp, dec_inp, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'price_data.csv'\n",
    "\n",
    "LRANGE = 80\n",
    "URANGE = 199\n",
    "RANGELENGTH = 20\n",
    "\n",
    "###################################################################\n",
    "#value of LRANGE, URANGE and RANGELENGTH has been decided from EDA#\n",
    "###################################################################\n",
    "\n",
    "def make_price_buckets(price):\n",
    "    return (min(max(price,LRANGE),URANGE)-LRANGE)//RANGELENGTH\n",
    "\n",
    "def prepare_data(path):\n",
    "    '''\n",
    "    takes path of the csv and prepares data for pytorch dataset function\n",
    "    '''\n",
    "    price_df = pd.read_csv(path)\n",
    "    '''\n",
    "    format date and extract day, month and weekday from date\n",
    "    '''\n",
    "    price_df['Date'] = pd.to_datetime(price_df['Date'],format='%m/%d/%Y')\n",
    "    price_df['weekday'], price_df['month'], price_df['day'] = price_df['Date'].dt.weekday, price_df['Date'].dt.month, price_df['Date'].dt.day\n",
    "    '''\n",
    "    make month and day 0 based index for the ease in using embedding\n",
    "    '''\n",
    "    price_df['month'] = price_df['month']-1\n",
    "    price_df['day'] = price_df['day'] - 1\n",
    "    '''\n",
    "    make price bucket for embedding prices into decoder. Bar the prices at 80-200. \n",
    "    '''\n",
    "    price_df['price_bucket'] = price_df['Price'].apply(lambda x: make_price_buckets(x))\n",
    "    return price_df\n",
    "price_df = prepare_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>price_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>99</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>93</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Price  weekday  month  day  price_bucket\n",
       "0 2012-01-01     99        6      0    0             0\n",
       "1 2012-01-02     95        0      0    1             0\n",
       "2 2012-01-03     96        1      0    2             0\n",
       "3 2012-01-04     95        2      0    3             0\n",
       "4 2012-01-05     93        3      0    4             0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_number_train:0 epoch_loss_train: 12719.77734375\n",
      "\n",
      "epoch_number_val:0 epoch_loss_val: 15803.1748046875\n",
      "\n",
      "epoch_number_train:1 epoch_loss_train: 12581.4345703125\n",
      "\n",
      "epoch_number_val:1 epoch_loss_val: 15649.9580078125\n",
      "\n",
      "epoch_number_train:2 epoch_loss_train: 12428.41796875\n",
      "\n",
      "epoch_number_val:2 epoch_loss_val: 15464.6044921875\n",
      "\n",
      "epoch_number_train:3 epoch_loss_train: 12254.0185546875\n",
      "\n",
      "epoch_number_val:3 epoch_loss_val: 15246.0830078125\n",
      "\n",
      "epoch_number_train:4 epoch_loss_train: 12047.9501953125\n",
      "\n",
      "epoch_number_val:4 epoch_loss_val: 14998.2373046875\n",
      "\n",
      "epoch_number_train:5 epoch_loss_train: 11819.4189453125\n",
      "\n",
      "epoch_number_val:5 epoch_loss_val: 14721.0625\n",
      "\n",
      "epoch_number_train:6 epoch_loss_train: 11564.4052734375\n",
      "\n",
      "epoch_number_val:6 epoch_loss_val: 14414.3701171875\n",
      "\n",
      "epoch_number_train:7 epoch_loss_train: 11286.0595703125\n",
      "\n",
      "epoch_number_val:7 epoch_loss_val: 14083.6826171875\n",
      "\n",
      "epoch_number_train:8 epoch_loss_train: 10985.4462890625\n",
      "\n",
      "epoch_number_val:8 epoch_loss_val: 13722.9150390625\n",
      "\n",
      "epoch_number_train:9 epoch_loss_train: 10665.7451171875\n",
      "\n",
      "epoch_number_val:9 epoch_loss_val: 13346.3427734375\n",
      "\n",
      "epoch_number_train:10 epoch_loss_train: 10330.5546875\n",
      "\n",
      "epoch_number_val:10 epoch_loss_val: 12944.9521484375\n",
      "\n",
      "epoch_number_train:11 epoch_loss_train: 9976.087890625\n",
      "\n",
      "epoch_number_val:11 epoch_loss_val: 12528.9912109375\n",
      "\n",
      "epoch_number_train:12 epoch_loss_train: 9610.0126953125\n",
      "\n",
      "epoch_number_val:12 epoch_loss_val: 12092.3544921875\n",
      "\n",
      "epoch_number_train:13 epoch_loss_train: 9225.99609375\n",
      "\n",
      "epoch_number_val:13 epoch_loss_val: 11641.2841796875\n",
      "\n",
      "epoch_number_train:14 epoch_loss_train: 8835.9033203125\n",
      "\n",
      "epoch_number_val:14 epoch_loss_val: 11184.2890625\n",
      "\n",
      "epoch_number_train:15 epoch_loss_train: 8434.681640625\n",
      "\n",
      "epoch_number_val:15 epoch_loss_val: 10697.8251953125\n",
      "\n",
      "epoch_number_train:16 epoch_loss_train: 8025.78564453125\n",
      "\n",
      "epoch_number_val:16 epoch_loss_val: 10229.548828125\n",
      "\n",
      "epoch_number_train:17 epoch_loss_train: 7613.22216796875\n",
      "\n",
      "epoch_number_val:17 epoch_loss_val: 9721.3271484375\n",
      "\n",
      "epoch_number_train:18 epoch_loss_train: 7187.43701171875\n",
      "\n",
      "epoch_number_val:18 epoch_loss_val: 9232.3271484375\n",
      "\n",
      "epoch_number_train:19 epoch_loss_train: 6769.70068359375\n",
      "\n",
      "epoch_number_val:19 epoch_loss_val: 8724.193359375\n",
      "\n",
      "epoch_number_train:20 epoch_loss_train: 6346.779296875\n",
      "\n",
      "epoch_number_val:20 epoch_loss_val: 8228.224609375\n",
      "\n",
      "epoch_number_train:21 epoch_loss_train: 5935.1015625\n",
      "\n",
      "epoch_number_val:21 epoch_loss_val: 7729.73193359375\n",
      "\n",
      "epoch_number_train:22 epoch_loss_train: 5515.03564453125\n",
      "\n",
      "epoch_number_val:22 epoch_loss_val: 7221.29736328125\n",
      "\n",
      "epoch_number_train:23 epoch_loss_train: 5104.83349609375\n",
      "\n",
      "epoch_number_val:23 epoch_loss_val: 6736.0078125\n",
      "\n",
      "epoch_number_train:24 epoch_loss_train: 4714.0068359375\n",
      "\n",
      "epoch_number_val:24 epoch_loss_val: 6258.61181640625\n",
      "\n",
      "epoch_number_train:25 epoch_loss_train: 4315.603515625\n",
      "\n",
      "epoch_number_val:25 epoch_loss_val: 5761.57421875\n",
      "\n",
      "epoch_number_train:26 epoch_loss_train: 3924.734130859375\n",
      "\n",
      "epoch_number_val:26 epoch_loss_val: 5295.49462890625\n",
      "\n",
      "epoch_number_train:27 epoch_loss_train: 3568.63671875\n",
      "\n",
      "epoch_number_val:27 epoch_loss_val: 4857.64404296875\n",
      "\n",
      "epoch_number_train:28 epoch_loss_train: 3216.0185546875\n",
      "\n",
      "epoch_number_val:28 epoch_loss_val: 4402.38818359375\n",
      "\n",
      "epoch_number_train:29 epoch_loss_train: 2864.3359375\n",
      "\n",
      "epoch_number_val:29 epoch_loss_val: 3962.334228515625\n",
      "\n",
      "epoch_number_train:30 epoch_loss_train: 2539.48583984375\n",
      "\n",
      "epoch_number_val:30 epoch_loss_val: 3555.306640625\n",
      "\n",
      "epoch_number_train:31 epoch_loss_train: 2240.27587890625\n",
      "\n",
      "epoch_number_val:31 epoch_loss_val: 3169.551513671875\n",
      "\n",
      "epoch_number_train:32 epoch_loss_train: 1959.9068603515625\n",
      "\n",
      "epoch_number_val:32 epoch_loss_val: 2800.592041015625\n",
      "\n",
      "epoch_number_train:33 epoch_loss_train: 1695.40869140625\n",
      "\n",
      "epoch_number_val:33 epoch_loss_val: 2439.072998046875\n",
      "\n",
      "epoch_number_train:34 epoch_loss_train: 1445.517578125\n",
      "\n",
      "epoch_number_val:34 epoch_loss_val: 2098.104736328125\n",
      "\n",
      "epoch_number_train:35 epoch_loss_train: 1226.404052734375\n",
      "\n",
      "epoch_number_val:35 epoch_loss_val: 1798.0718994140625\n",
      "\n",
      "epoch_number_train:36 epoch_loss_train: 1038.950439453125\n",
      "\n",
      "epoch_number_val:36 epoch_loss_val: 1523.42578125\n",
      "\n",
      "epoch_number_train:37 epoch_loss_train: 868.9560546875\n",
      "\n",
      "epoch_number_val:37 epoch_loss_val: 1261.4322509765625\n",
      "\n",
      "epoch_number_train:38 epoch_loss_train: 725.7235717773438\n",
      "\n",
      "epoch_number_val:38 epoch_loss_val: 1034.4063720703125\n",
      "\n",
      "epoch_number_train:39 epoch_loss_train: 609.2196655273438\n",
      "\n",
      "epoch_number_val:39 epoch_loss_val: 830.6262817382812\n",
      "\n",
      "epoch_number_train:40 epoch_loss_train: 514.5935668945312\n",
      "\n",
      "epoch_number_val:40 epoch_loss_val: 645.80517578125\n",
      "\n",
      "epoch_number_train:41 epoch_loss_train: 445.7666015625\n",
      "\n",
      "epoch_number_val:41 epoch_loss_val: 486.304931640625\n",
      "\n",
      "epoch_number_train:42 epoch_loss_train: 405.4311828613281\n",
      "\n",
      "epoch_number_val:42 epoch_loss_val: 357.38525390625\n",
      "\n",
      "epoch_number_train:43 epoch_loss_train: 394.4993896484375\n",
      "\n",
      "epoch_number_val:43 epoch_loss_val: 259.8362731933594\n",
      "\n",
      "epoch_number_train:44 epoch_loss_train: 411.6796569824219\n",
      "\n",
      "epoch_number_val:44 epoch_loss_val: 191.09190368652344\n",
      "\n",
      "epoch_number_train:45 epoch_loss_train: 455.11767578125\n",
      "\n",
      "epoch_number_val:45 epoch_loss_val: 151.18211364746094\n",
      "\n",
      "epoch_number_train:46 epoch_loss_train: 523.2185668945312\n",
      "\n",
      "epoch_number_val:46 epoch_loss_val: 136.78846740722656\n",
      "\n",
      "epoch_number_train:47 epoch_loss_train: 612.5723266601562\n",
      "\n",
      "epoch_number_val:47 epoch_loss_val: 146.9822235107422\n",
      "\n",
      "epoch_number_train:48 epoch_loss_train: 716.7916870117188\n",
      "\n",
      "epoch_number_val:48 epoch_loss_val: 176.16978454589844\n",
      "\n",
      "epoch_number_train:49 epoch_loss_train: 821.930908203125\n",
      "\n",
      "epoch_number_val:49 epoch_loss_val: 207.5365753173828\n",
      "\n",
      "epoch_number_train:50 epoch_loss_train: 908.16357421875\n",
      "\n",
      "epoch_number_val:50 epoch_loss_val: 232.3217315673828\n",
      "\n",
      "epoch_number_train:51 epoch_loss_train: 952.7704467773438\n",
      "\n",
      "epoch_number_val:51 epoch_loss_val: 243.4544219970703\n",
      "\n",
      "epoch_number_train:52 epoch_loss_train: 965.348876953125\n",
      "\n",
      "epoch_number_val:52 epoch_loss_val: 249.6645050048828\n",
      "\n",
      "epoch_number_train:53 epoch_loss_train: 979.11962890625\n",
      "\n",
      "epoch_number_val:53 epoch_loss_val: 257.9809265136719\n",
      "\n",
      "epoch_number_train:54 epoch_loss_train: 1014.770751953125\n",
      "\n",
      "epoch_number_val:54 epoch_loss_val: 277.4317626953125\n",
      "\n",
      "epoch_number_train:55 epoch_loss_train: 1048.1910400390625\n",
      "\n",
      "epoch_number_val:55 epoch_loss_val: 286.5503845214844\n",
      "\n",
      "epoch_number_train:56 epoch_loss_train: 1056.8228759765625\n",
      "\n",
      "epoch_number_val:56 epoch_loss_val: 285.35064697265625\n",
      "\n",
      "epoch_number_train:57 epoch_loss_train: 1042.2166748046875\n",
      "\n",
      "epoch_number_val:57 epoch_loss_val: 274.3903503417969\n",
      "\n",
      "epoch_number_train:58 epoch_loss_train: 1002.0554809570312\n",
      "\n",
      "epoch_number_val:58 epoch_loss_val: 243.51312255859375\n",
      "\n",
      "epoch_number_train:59 epoch_loss_train: 954.195068359375\n",
      "\n",
      "epoch_number_val:59 epoch_loss_val: 237.84893798828125\n",
      "\n",
      "epoch_number_train:60 epoch_loss_train: 888.951171875\n",
      "\n",
      "epoch_number_val:60 epoch_loss_val: 203.2278289794922\n",
      "\n",
      "epoch_number_train:61 epoch_loss_train: 816.4988403320312\n",
      "\n",
      "epoch_number_val:61 epoch_loss_val: 201.5927734375\n",
      "\n",
      "epoch_number_train:62 epoch_loss_train: 748.3853149414062\n",
      "\n",
      "epoch_number_val:62 epoch_loss_val: 185.4540252685547\n",
      "\n",
      "epoch_number_train:63 epoch_loss_train: 682.3573608398438\n",
      "\n",
      "epoch_number_val:63 epoch_loss_val: 205.1970977783203\n",
      "\n",
      "epoch_number_train:64 epoch_loss_train: 631.8387451171875\n",
      "\n",
      "epoch_number_val:64 epoch_loss_val: 247.0088653564453\n",
      "\n",
      "epoch_number_train:65 epoch_loss_train: 599.69482421875\n",
      "\n",
      "epoch_number_val:65 epoch_loss_val: 314.3886413574219\n",
      "\n",
      "epoch_number_train:66 epoch_loss_train: 587.4227294921875\n",
      "\n",
      "epoch_number_val:66 epoch_loss_val: 381.7113037109375\n",
      "\n",
      "epoch_number_train:67 epoch_loss_train: 601.15234375\n",
      "\n",
      "epoch_number_val:67 epoch_loss_val: 492.5279846191406\n",
      "\n",
      "epoch_number_train:68 epoch_loss_train: 611.0403442382812\n",
      "\n",
      "epoch_number_val:68 epoch_loss_val: 589.9009399414062\n",
      "\n",
      "epoch_number_train:69 epoch_loss_train: 610.9089965820312\n",
      "\n",
      "epoch_number_val:69 epoch_loss_val: 554.2389526367188\n",
      "\n",
      "epoch_number_train:70 epoch_loss_train: 601.6513061523438\n",
      "\n",
      "epoch_number_val:70 epoch_loss_val: 570.3607788085938\n",
      "\n",
      "epoch_number_train:71 epoch_loss_train: 578.6304321289062\n",
      "\n",
      "epoch_number_val:71 epoch_loss_val: 458.123046875\n",
      "\n",
      "epoch_number_train:72 epoch_loss_train: 566.0476684570312\n",
      "\n",
      "epoch_number_val:72 epoch_loss_val: 441.3457336425781\n",
      "\n",
      "epoch_number_train:73 epoch_loss_train: 563.2446899414062\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_number_val:73 epoch_loss_val: 400.1477966308594\n",
      "\n",
      "epoch_number_train:74 epoch_loss_train: 571.1250610351562\n",
      "\n",
      "epoch_number_val:74 epoch_loss_val: 358.8323974609375\n",
      "\n",
      "epoch_number_train:75 epoch_loss_train: 557.0470581054688\n",
      "\n",
      "epoch_number_val:75 epoch_loss_val: 384.0614318847656\n",
      "\n",
      "epoch_number_train:76 epoch_loss_train: 581.67919921875\n",
      "\n",
      "epoch_number_val:76 epoch_loss_val: 328.6908874511719\n",
      "\n",
      "epoch_number_train:77 epoch_loss_train: 596.081787109375\n",
      "\n",
      "epoch_number_val:77 epoch_loss_val: 311.36163330078125\n",
      "\n",
      "epoch_number_train:78 epoch_loss_train: 612.5618896484375\n",
      "\n",
      "epoch_number_val:78 epoch_loss_val: 337.3544006347656\n",
      "\n",
      "epoch_number_train:79 epoch_loss_train: 606.1331176757812\n",
      "\n",
      "epoch_number_val:79 epoch_loss_val: 360.4973449707031\n",
      "\n",
      "epoch_number_train:80 epoch_loss_train: 592.9929809570312\n",
      "\n",
      "epoch_number_val:80 epoch_loss_val: 370.83056640625\n",
      "\n",
      "epoch_number_train:81 epoch_loss_train: 585.9598999023438\n",
      "\n",
      "epoch_number_val:81 epoch_loss_val: 367.748779296875\n",
      "\n",
      "epoch_number_train:82 epoch_loss_train: 597.5128784179688\n",
      "\n",
      "epoch_number_val:82 epoch_loss_val: 425.8854064941406\n",
      "\n",
      "epoch_number_train:83 epoch_loss_train: 593.5584716796875\n",
      "\n",
      "epoch_number_val:83 epoch_loss_val: 438.5365295410156\n",
      "\n",
      "epoch_number_train:84 epoch_loss_train: 592.9022827148438\n",
      "\n",
      "epoch_number_val:84 epoch_loss_val: 437.3081970214844\n",
      "\n",
      "epoch_number_train:85 epoch_loss_train: 593.8276977539062\n",
      "\n",
      "epoch_number_val:85 epoch_loss_val: 514.2356567382812\n",
      "\n",
      "epoch_number_train:86 epoch_loss_train: 587.3203735351562\n",
      "\n",
      "epoch_number_val:86 epoch_loss_val: 447.61865234375\n",
      "\n",
      "epoch_number_train:87 epoch_loss_train: 587.8292236328125\n",
      "\n",
      "epoch_number_val:87 epoch_loss_val: 518.6692504882812\n",
      "\n",
      "epoch_number_train:88 epoch_loss_train: 587.1765747070312\n",
      "\n",
      "epoch_number_val:88 epoch_loss_val: 502.316162109375\n",
      "\n",
      "epoch_number_train:89 epoch_loss_train: 592.5599365234375\n",
      "\n",
      "epoch_number_val:89 epoch_loss_val: 441.9353942871094\n",
      "\n",
      "epoch_number_train:90 epoch_loss_train: 596.6660766601562\n",
      "\n",
      "epoch_number_val:90 epoch_loss_val: 415.3606262207031\n",
      "\n",
      "epoch_number_train:91 epoch_loss_train: 595.0723266601562\n",
      "\n",
      "epoch_number_val:91 epoch_loss_val: 438.6788024902344\n",
      "\n",
      "epoch_number_train:92 epoch_loss_train: 586.2597045898438\n",
      "\n",
      "epoch_number_val:92 epoch_loss_val: 474.4599609375\n",
      "\n",
      "epoch_number_train:93 epoch_loss_train: 597.2402954101562\n",
      "\n",
      "epoch_number_val:93 epoch_loss_val: 445.5714111328125\n",
      "\n",
      "epoch_number_train:94 epoch_loss_train: 605.9778442382812\n",
      "\n",
      "epoch_number_val:94 epoch_loss_val: 408.5201416015625\n",
      "\n",
      "epoch_number_train:95 epoch_loss_train: 600.4414672851562\n",
      "\n",
      "epoch_number_val:95 epoch_loss_val: 461.3255310058594\n",
      "\n",
      "epoch_number_train:96 epoch_loss_train: 606.6195678710938\n",
      "\n",
      "epoch_number_val:96 epoch_loss_val: 455.6864929199219\n",
      "\n",
      "epoch_number_train:97 epoch_loss_train: 599.5969848632812\n",
      "\n",
      "epoch_number_val:97 epoch_loss_val: 458.0147399902344\n",
      "\n",
      "epoch_number_train:98 epoch_loss_train: 593.2467651367188\n",
      "\n",
      "epoch_number_val:98 epoch_loss_val: 472.8377380371094\n",
      "\n",
      "epoch_number_train:99 epoch_loss_train: 586.3591918945312\n",
      "\n",
      "epoch_number_val:99 epoch_loss_val: 506.0475158691406\n",
      "\n",
      "epoch_number_train:100 epoch_loss_train: 595.267822265625\n",
      "\n",
      "epoch_number_val:100 epoch_loss_val: 535.7804565429688\n",
      "\n",
      "epoch_number_train:101 epoch_loss_train: 591.5311279296875\n",
      "\n",
      "epoch_number_val:101 epoch_loss_val: 502.53955078125\n",
      "\n",
      "epoch_number_train:102 epoch_loss_train: 599.939208984375\n",
      "\n",
      "epoch_number_val:102 epoch_loss_val: 527.52978515625\n",
      "\n",
      "epoch_number_train:103 epoch_loss_train: 600.2262573242188\n",
      "\n",
      "epoch_number_val:103 epoch_loss_val: 559.957763671875\n",
      "\n",
      "epoch_number_train:104 epoch_loss_train: 586.1244506835938\n",
      "\n",
      "epoch_number_val:104 epoch_loss_val: 513.3523559570312\n",
      "\n",
      "epoch_number_train:105 epoch_loss_train: 600.7202758789062\n",
      "\n",
      "epoch_number_val:105 epoch_loss_val: 519.166259765625\n",
      "\n",
      "epoch_number_train:106 epoch_loss_train: 599.5488891601562\n",
      "\n",
      "epoch_number_val:106 epoch_loss_val: 545.3449096679688\n",
      "\n",
      "epoch_number_train:107 epoch_loss_train: 596.9267578125\n",
      "\n",
      "epoch_number_val:107 epoch_loss_val: 460.1221618652344\n",
      "\n",
      "epoch_number_train:108 epoch_loss_train: 600.2427368164062\n",
      "\n",
      "epoch_number_val:108 epoch_loss_val: 515.8563842773438\n",
      "\n",
      "epoch_number_train:109 epoch_loss_train: 599.2979125976562\n",
      "\n",
      "epoch_number_val:109 epoch_loss_val: 513.9364624023438\n",
      "\n",
      "epoch_number_train:110 epoch_loss_train: 598.8709716796875\n",
      "\n",
      "epoch_number_val:110 epoch_loss_val: 422.55322265625\n",
      "\n",
      "epoch_number_train:111 epoch_loss_train: 620.0651245117188\n",
      "\n",
      "epoch_number_val:111 epoch_loss_val: 473.4363098144531\n",
      "\n",
      "epoch_number_train:112 epoch_loss_train: 610.4984741210938\n",
      "\n",
      "epoch_number_val:112 epoch_loss_val: 418.7611083984375\n",
      "\n",
      "epoch_number_train:113 epoch_loss_train: 618.2778930664062\n",
      "\n",
      "epoch_number_val:113 epoch_loss_val: 465.1119079589844\n",
      "\n",
      "epoch_number_train:114 epoch_loss_train: 625.1415405273438\n",
      "\n",
      "epoch_number_val:114 epoch_loss_val: 491.9941711425781\n",
      "\n",
      "epoch_number_train:115 epoch_loss_train: 607.8372192382812\n",
      "\n",
      "epoch_number_val:115 epoch_loss_val: 476.1471252441406\n",
      "\n",
      "epoch_number_train:116 epoch_loss_train: 619.5949096679688\n",
      "\n",
      "epoch_number_val:116 epoch_loss_val: 521.7015380859375\n",
      "\n",
      "epoch_number_train:117 epoch_loss_train: 614.5564575195312\n",
      "\n",
      "epoch_number_val:117 epoch_loss_val: 501.7457580566406\n",
      "\n",
      "epoch_number_train:118 epoch_loss_train: 626.6567993164062\n",
      "\n",
      "epoch_number_val:118 epoch_loss_val: 482.7268981933594\n",
      "\n",
      "epoch_number_train:119 epoch_loss_train: 604.760986328125\n",
      "\n",
      "epoch_number_val:119 epoch_loss_val: 555.1807250976562\n",
      "\n",
      "epoch_number_train:120 epoch_loss_train: 607.1812133789062\n",
      "\n",
      "epoch_number_val:120 epoch_loss_val: 478.1939392089844\n",
      "\n",
      "epoch_number_train:121 epoch_loss_train: 608.5494384765625\n",
      "\n",
      "epoch_number_val:121 epoch_loss_val: 662.8172607421875\n",
      "\n",
      "epoch_number_train:122 epoch_loss_train: 622.84619140625\n",
      "\n",
      "epoch_number_val:122 epoch_loss_val: 501.130859375\n",
      "\n",
      "epoch_number_train:123 epoch_loss_train: 603.0015258789062\n",
      "\n",
      "epoch_number_val:123 epoch_loss_val: 472.695556640625\n",
      "\n",
      "epoch_number_train:124 epoch_loss_train: 608.3573608398438\n",
      "\n",
      "epoch_number_val:124 epoch_loss_val: 432.6701965332031\n",
      "\n",
      "epoch_number_train:125 epoch_loss_train: 611.8773803710938\n",
      "\n",
      "epoch_number_val:125 epoch_loss_val: 430.5194396972656\n",
      "\n",
      "epoch_number_train:126 epoch_loss_train: 628.7481689453125\n",
      "\n",
      "epoch_number_val:126 epoch_loss_val: 408.1549377441406\n",
      "\n",
      "epoch_number_train:127 epoch_loss_train: 631.1734619140625\n",
      "\n",
      "epoch_number_val:127 epoch_loss_val: 500.6883239746094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "d_model = 4\n",
    "nhead = 2\n",
    "dim_feedforward = 32\n",
    "num_weekday = 7\n",
    "num_months = 12\n",
    "num_days = 31\n",
    "num_price_bucket = 6\n",
    "EPOCHS = 128\n",
    "\n",
    "price_dataset = priceDataset(price_df)\n",
    "\n",
    "validation_split = 0.05\n",
    "indices = list(range(len(price_dataset)))\n",
    "split = int(len(indices)*validation_split)\n",
    "\n",
    "train_split, val_split = indices[:-split], indices[-split:]\n",
    "\n",
    "model = TransformerModel(seq_len=SEQLEN, d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, num_weekday=num_weekday, num_months=num_months, num_days=num_days, num_price_bucket=num_price_bucket)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "price_train_dataloader = DataLoader(price_dataset, batch_size=30, num_workers = 12, sampler = train_split)\n",
    "price_val_dataloader = DataLoader(price_dataset, batch_size=30, num_workers=6, sampler=val_split)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    cnt = 1\n",
    "    for enc_inp, dec_inp, response in price_train_dataloader:\n",
    "        enc_inp = enc_inp.to(device)\n",
    "        dec_inp = dec_inp.to(device)\n",
    "        response = response.to(device)\n",
    "        response = response.transpose(0,1)\n",
    "        output = model(enc_inp, dec_inp)\n",
    "        output = output.squeeze()\n",
    "        loss = loss_fn(response, output)\n",
    "        epoch_loss+=loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cnt+=1\n",
    "    print(f'epoch_number_train:{epoch} epoch_loss_train: {epoch_loss/cnt}\\n')\n",
    "    with torch.no_grad():\n",
    "        epoch_loss_val = 0\n",
    "        cnt=0\n",
    "        for enc_inp, dec_inp, response in price_val_dataloader:\n",
    "            response = response.transpose(0,1)\n",
    "            output = model(enc_inp, dec_inp)\n",
    "            output = output.squeeze()\n",
    "            loss = loss_fn(response, output)\n",
    "            epoch_loss_val += loss\n",
    "            cnt+=1\n",
    "        \n",
    "    print(f'epoch_number_val:{epoch} epoch_loss_val: {epoch_loss_val/cnt}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = price_df.iloc[-6:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>price_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Price  weekday  month  day  price_bucket\n",
       "0 2016-01-11    139        0      0   10             2\n",
       "1 2016-01-12    147        1      0   11             3\n",
       "2 2016-01-13    150        2      0   12             3\n",
       "3 2016-01-14    148        3      0   13             3\n",
       "4 2016-01-15    149        4      0   14             3\n",
       "5 2016-01-16    147        5      0   15             3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2020-02-01','2020-02-29',freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame({'Date':dates})\n",
    "test_df = pd.concat([test_df, df_temp], ignore_index=True).reindex(columns=test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['weekday'], test_df['month'], test_df['day'] = test_df['Date'].dt.weekday, test_df['Date'].dt.month, test_df['Date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = test_df.shape[0]\n",
    "default_price_bucket = price_df['price_bucket'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getitem(test_df, index):\n",
    "    enc_inp = torch.tensor(test_df[['month', 'day', 'weekday']][index:index+SEQLEN].values)\n",
    "    if index:\n",
    "        dec_inp = torch.tensor(test_df['price_bucket'][index-1:index+SEQLEN-1].values, dtype = torch.long)\n",
    "    else:\n",
    "        dec_inp = torch.tensor(np.append(default_price_bucket,test_df['price_bucket'][index:index+SEQLEN-1]), dtype = torch.long)\n",
    "    return enc_inp, dec_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_len-SEQLEN+1):\n",
    "    enc_inp, dec_inp = getitem(test_df, i)\n",
    "    enc_inp = enc_inp.unsqueeze(0)\n",
    "    dec_inp = dec_inp.unsqueeze(0)\n",
    "    response = model(enc_inp, dec_inp)\n",
    "    response = response[6].item()\n",
    "    test_df.loc[i+SEQLEN-1,'Price'] = response\n",
    "    test_df.loc[i+SEQLEN-1, 'price_bucket'] = make_price_buckets(response)\n",
    "    #breakpoint()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Price'] = test_df['Price'].round(2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.loc[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>price_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>106.41</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>110.18</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>117.59</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>109.98</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>109.70</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-02-06</td>\n",
       "      <td>109.65</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>111.42</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-02-08</td>\n",
       "      <td>109.95</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>130.78</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>112.14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Price  weekday  month  day  price_bucket\n",
       "6  2020-02-01  106.41        5      2    1           1.0\n",
       "7  2020-02-02  110.18        6      2    2           1.0\n",
       "8  2020-02-03  117.59        0      2    3           1.0\n",
       "9  2020-02-04  109.98        1      2    4           1.0\n",
       "10 2020-02-05  109.70        2      2    5           1.0\n",
       "11 2020-02-06  109.65        3      2    6           1.0\n",
       "12 2020-02-07  111.42        4      2    7           1.0\n",
       "13 2020-02-08  109.95        5      2    8           1.0\n",
       "14 2020-02-09  130.78        6      2    9           2.0\n",
       "15 2020-02-10  112.14        0      2   10           1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
